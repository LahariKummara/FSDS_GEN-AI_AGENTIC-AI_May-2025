{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8717cc73",
   "metadata": {},
   "source": [
    "\n",
    "# **spaCy for NLP**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02001079",
   "metadata": {},
   "source": [
    "**Introduction to spaCy**\n",
    "\n",
    "- spaCy is a free, open-source Python library for advanced Natural Language Processing (NLP).\n",
    "\n",
    "- It is not an API — meaning it won’t answer questions out of the box like ChatGPT, but it provides powerful tools to process and analyze text.\n",
    "\n",
    "- Key idea: Whatever we did using NLTK, we can also do (and often faster) with spaCy.\n",
    "\n",
    "- spaCy is designed for production use (speed + efficiency) and supports pre-trained language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89635cc6",
   "metadata": {},
   "source": [
    "# Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f53105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy (only once)\n",
    "#!pip install spacy\n",
    "\n",
    "# Download the English language model (small)\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "# For larger models:\n",
    "# en_core_web_md  (medium)\n",
    "# en_core_web_lg  (large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a13da5",
   "metadata": {},
   "source": [
    "# Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57774f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# Process the text into a spaCy Doc object\n",
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec88ccb",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f29419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "# Print individual tokens\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6bb89",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b49f0449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple : PROPN\n",
      "is : AUX\n",
      "looking : VERB\n",
      "at : ADP\n",
      "buying : VERB\n",
      "U.K. : PROPN\n",
      "startup : VERB\n",
      "for : ADP\n",
      "$ : SYM\n",
      "1 : NUM\n",
      "billion : NUM\n"
     ]
    }
   ],
   "source": [
    "# Print tokens with their POS tags\n",
    "for token in doc:\n",
    "    print(token.text, \":\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88dd27",
   "metadata": {},
   "source": [
    "## Lemmatization & Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300d3d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple : PROPN --> Apple | Dependency: nsubj\n",
      "is : AUX --> be | Dependency: aux\n",
      "looking : VERB --> look | Dependency: ROOT\n",
      "at : ADP --> at | Dependency: prep\n",
      "buying : VERB --> buy | Dependency: pcomp\n",
      "U.K. : PROPN --> U.K. | Dependency: nsubj\n",
      "startup : VERB --> startup | Dependency: ccomp\n",
      "for : ADP --> for | Dependency: prep\n",
      "$ : SYM --> $ | Dependency: quantmod\n",
      "1 : NUM --> 1 | Dependency: compound\n",
      "billion : NUM --> billion | Dependency: pobj\n"
     ]
    }
   ],
   "source": [
    "# Tokens with POS, Lemma (base form), and Dependency relation\n",
    "for token in doc:\n",
    "    print(token.text, \":\", token.pos_, \"-->\", token.lemma_, \"| Dependency:\", token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab11f93",
   "metadata": {},
   "source": [
    "## Extended Token Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "965af56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple : PROPN --> Apple | Dep: nsubj | Shape: Xxxxx | Alpha: True | Stopword: False\n",
      "is : AUX --> be | Dep: aux | Shape: xx | Alpha: True | Stopword: True\n",
      "looking : VERB --> look | Dep: ROOT | Shape: xxxx | Alpha: True | Stopword: False\n",
      "at : ADP --> at | Dep: prep | Shape: xx | Alpha: True | Stopword: True\n",
      "buying : VERB --> buy | Dep: pcomp | Shape: xxxx | Alpha: True | Stopword: False\n",
      "U.K. : PROPN --> U.K. | Dep: nsubj | Shape: X.X. | Alpha: False | Stopword: False\n",
      "startup : VERB --> startup | Dep: ccomp | Shape: xxxx | Alpha: True | Stopword: False\n",
      "for : ADP --> for | Dep: prep | Shape: xxx | Alpha: True | Stopword: True\n",
      "$ : SYM --> $ | Dep: quantmod | Shape: $ | Alpha: False | Stopword: False\n",
      "1 : NUM --> 1 | Dep: compound | Shape: d | Alpha: False | Stopword: False\n",
      "billion : NUM --> billion | Dep: pobj | Shape: xxxx | Alpha: True | Stopword: False\n"
     ]
    }
   ],
   "source": [
    "# All in one: POS, Lemma, Dependency, Shape, Alphabet check, Stop word check\n",
    "for token in doc:\n",
    "    print(token.text, \":\", token.pos_, \"-->\", token.lemma_, \"|\", \n",
    "          \"Dep:\", token.dep_, \"| Shape:\", token.shape_, \n",
    "          \"| Alpha:\", token.is_alpha, \"| Stopword:\", token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392aa8b",
   "metadata": {},
   "source": [
    "> Note — **spaCy does not do *everything* NLTK does**, but it covers almost all the **practical / production-level NLP tasks**. Let me break this down clearly for your notebook:\n",
    "\n",
    "# NLTK vs spaCy – Coverage\n",
    "\n",
    "## Things you can do with both NLTK & spaCy\n",
    "\n",
    "* **Tokenization** (word, sentence, paragraph)\n",
    "* **Part of Speech (POS) tagging**\n",
    "* **Lemmatization**\n",
    "* **Dependency Parsing**\n",
    "* **Named Entity Recognition (NER)**\n",
    "* **Stopword Removal**\n",
    "* **Word Similarity (using embeddings)**\n",
    "* **Text Classification (with training)**\n",
    "\n",
    "## Things spaCy does better than NLTK\n",
    "\n",
    "* **Speed** → much faster for large text.\n",
    "* **Pre-trained models** → spaCy has `en_core_web_sm`, `md`, `lg`.\n",
    "* **NER & Dependency Parsing** → built-in and more accurate.\n",
    "* **Integration with deep learning** → easily works with PyTorch, TensorFlow.\n",
    "* **Pipeline structure** → everything runs in a single `nlp()` call.\n",
    "\n",
    "## Things NLTK does that spaCy does not (or less focused)\n",
    "\n",
    "* **Corpora access** (movie reviews, Brown corpus, WordNet, etc.).\n",
    "* **Linguistic resources** (CFG parsing, grammar trees, etc.).\n",
    "* **Educational focus** (helps students learn concepts).\n",
    "* **Rule-based text processing** (regex tokenizer, stemmers).\n",
    "* **Language coverage** → NLTK supports more small academic datasets.\n",
    "\n",
    "\n",
    "## Key Points:\n",
    "    \n",
    "* **If you want research, corpora, and learning basics → use NLTK.**\n",
    "* **If you want production-ready pipelines, speed, and accuracy → use spaCy.**\n",
    "* In real projects → many people **learn with NLTK, but deploy with spaCy**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f048e7",
   "metadata": {},
   "source": [
    "## Quick Example: Same task in NLTK vs spaCy\n",
    "\n",
    "### Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b76e95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02570060",
   "metadata": {},
   "source": [
    "### Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "670664cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769d29c",
   "metadata": {},
   "source": [
    "## **Advantages of spaCy**\n",
    "\n",
    "- Fast & efficient for large-scale NLP tasks.\n",
    "\n",
    "- Provides state-of-the-art models for POS tagging, NER, dependency parsing.\n",
    "\n",
    "- Easy to integrate with deep learning frameworks (TensorFlow, PyTorch).\n",
    "\n",
    "- Production-ready with optimized pipelines.\n",
    "\n",
    "## **Disadvantages of spaCy**\n",
    "\n",
    "- Less suitable for linguistic research (NLTK has more linguistic corpora).\n",
    "\n",
    "- Requires downloading large models for advanced features.\n",
    "\n",
    "- Fewer built-in datasets compared to NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17e5e3",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- spaCy is a modern alternative to NLTK, better suited for real-world applications.\n",
    "\n",
    "- It makes text processing pipelines faster and simpler.\n",
    "\n",
    "- Great for tasks like Tokenization, POS tagging, Lemmatization, Named Entity Recognition (NER), and Dependency Parsing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
